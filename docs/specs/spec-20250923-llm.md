# Tech Spec — LLM

**Version:** v1.0.0
**File:** docs/specs/spec-20250923-llm.md
**Status:** Draft
**PRD:** `prd-20250923.md`
**Contract Versions:** Prompt Set v1.0 • Model API v1.0 • Evaluation v1.0

## Overview & Goals

Design and implement a robust LLM integration system for job description parsing, artifact-to-skill matching, and CV/cover letter content generation. Target ≥95% generation success rate, ≤30s end-to-end generation time, and ≥8/10 user quality ratings through prompt engineering, model selection, and evaluation frameworks.

Links to latest PRD: `docs/prds/prd-20250923.md`

## Architecture (Detailed)

### Topology (frameworks)

```
┌─────────────────────────────────────────────────────────────────────────┐
│                    LLM Orchestration Layer                              │
│  ┌─────────────────┬─────────────────┬─────────────────┬─────────────┐  │
│  │   Prompt        │   Model         │   Response      │   Quality   │  │
│  │   Manager       │   Router        │   Parser        │  Validator  │  │
│  │                 │                 │                 │             │  │
│  └─────────────────┼─────────────────┼─────────────────┼─────────────┘  │
└──────────────────┬─┼─────────────────┼─────────────────┼─────────────────┘
                   │ │                 │                 │
┌──────────────────▼─▼─────────────────▼─────────────────▼─────────────────┐
│                      Task Processors                                    │
│  ┌─────────────────┬─────────────────┬─────────────────┬─────────────┐  │
│  │   Job Desc      │   Skill         │     CV          │   Cover     │  │
│  │   Parser        │   Matcher       │   Generator     │  Letter     │  │
│  │                 │                 │                 │ Generator   │  │
│  └─────────────────┼─────────────────┼─────────────────┼─────────────┘  │
└──────────────────┬─┼─────────────────┼─────────────────┼─────────────────┘
                   │ │                 │                 │
       ┌───────────▼─▼─────────────────▼─────────────────▼───────────┐
       │                    Provider Abstraction                     │
       │  ┌─────────────┬─────────────┬─────────────────────────────┐ │
       │  │   OpenAI    │  Anthropic  │      Fallback Provider      │ │
       │  │   Client    │   Client    │      (Local/HuggingFace)    │ │
       │  └─────────────┴─────────────┴─────────────────────────────┘ │
       └─────────────────────┬───────────────────────────────────────┘
                             │ HTTP API Calls
       ┌─────────────────────▼───────────────────────────────────────┐
       │                External LLM APIs                           │
       │  ┌──────────────┬──────────────┬────────────────────────┐   │
       │  │   OpenAI     │  Anthropic   │     Azure OpenAI       │   │
       │  │   GPT-4      │   Claude     │      (Backup)          │   │
       │  └──────────────┼──────────────┼────────────────────────┘   │
       └─────────────────┼──────────────┼────────────────────────────┘
                         │              │
┌────────────────────────▼──────────────▼────────────────────────────────┐
│                    Evaluation System                                   │
│  ┌─────────────┬─────────────┬─────────────┬─────────────────────────┐ │
│  │  Golden     │  Quality    │ Performance │     A/B Testing         │ │
│  │  Dataset    │  Metrics    │  Monitor    │     Framework           │ │
│  │  Manager    │ Calculator  │             │                         │ │
│  └─────────────┴─────────────┴─────────────┴─────────────────────────┘ │
└───────────────────────────────────────────────────────────────────────┘
```

### Component Inventory

| Component | Framework/Runtime | Purpose | Interfaces (in/out) | Depends On | Scale/HA | Owner |
|-----------|------------------|---------|-------------------|------------|----------|-------|
| Prompt Manager | Python + Jinja2 + uv | Template management, version control | In: Task type + data; Out: Formatted prompts | Template storage, DB | Stateless, cacheable | ML/Backend |
| Model Router | Python + asyncio + uv | Load balancing, failover between providers | In: Requests; Out: Routed calls | Provider clients | Auto-scale, circuit breaker | ML/Backend |
| Response Parser | Python + Pydantic + uv | Structure and validate LLM outputs | In: Raw responses; Out: Typed objects | Pydantic models | Stateless, fast | ML/Backend |
| Quality Validator | Python + Rule Engine + uv | Content quality checks, safety filters | In: Generated content; Out: Quality scores | Evaluation metrics | CPU-intensive scaling | ML/Backend |
| Job Desc Parser | LLM + Extraction Logic | Parse requirements, skills, company info | In: Raw JD text; Out: Structured data | LLM providers, skill taxonomy | Async processing | ML/Backend |
| Skill Matcher | LLM + Vector Similarity | Match artifacts to job requirements | In: Artifacts + JD; Out: Relevance scores | Embedding models, vector DB | Memory-intensive | ML/Backend |
| CV Generator | LLM + Template Engine | Generate tailored CV content | In: Matched artifacts + template; Out: CV sections | LLM providers, templates | Rate-limited by API | ML/Backend |
| Cover Letter Generator | LLM + Personalization | Generate targeted cover letters | In: JD + artifacts; Out: Cover letter | LLM providers | Rate-limited by API | ML/Backend |
| Provider Clients | HTTP + Retry Logic + uv | Abstract API calls to LLM services | In: Prompts; Out: Completions | External APIs | Connection pooling | ML/Backend |
| Evaluation System | Python + MLflow + uv | Track metrics, A/B tests, golden sets | In: Outputs + labels; Out: Quality metrics | MLflow, golden datasets | Batch processing | ML/Backend |

## Interfaces & Data Contracts

### Prompt Management System
```python
# Prompt Template Structure
@dataclass
class PromptTemplate:
    id: str
    name: str
    version: str
    task_type: str  # 'parse_jd', 'match_skills', 'generate_cv', 'generate_cover_letter'
    template: str
    input_schema: Dict[str, Any]
    output_schema: Dict[str, Any]
    model_params: Dict[str, Any]
    created_at: datetime
    is_active: bool

# Job Description Parsing
JD_PARSING_PROMPT = """
Parse the following job description and extract structured information:

Job Description:
{{job_description}}

Extract the following information in JSON format:
{
  "role_title": "string",
  "company_info": {
    "name": "string",
    "industry": "string",
    "size": "startup|small|medium|large|enterprise"
  },
  "requirements": {
    "must_have_skills": ["skill1", "skill2"],
    "nice_to_have_skills": ["skill3", "skill4"],
    "experience_years": "number|range",
    "education": "string",
    "certifications": ["cert1", "cert2"]
  },
  "responsibilities": ["responsibility1", "responsibility2"],
  "company_values": ["value1", "value2"],
  "benefits": ["benefit1", "benefit2"]
}

Be specific and extract exact skill names. Normalize similar skills (e.g., "React.js" -> "React").
"""

# CV Generation Prompt
CV_GENERATION_PROMPT = """
Generate professional CV content based on the provided artifacts and job requirements.

Job Requirements:
{{job_requirements}}

User Artifacts:
{% for artifact in artifacts %}
- Title: {{artifact.title}}
- Description: {{artifact.description}}
- Technologies: {{artifact.technologies|join(", ")}}
- Evidence: {{artifact.evidence_links|length}} links available
- Relevance Score: {{artifact.relevance_score}}/10
{% endfor %}

Generate CV sections in JSON format:
{
  "professional_summary": "2-3 sentence summary highlighting relevant experience",
  "key_skills": ["skill1", "skill2", "skill3"],
  "experience": [
    {
      "title": "Based on artifact",
      "organization": "From artifact or inferred",
      "duration": "From artifact dates",
      "achievements": [
        "Quantified achievement with metrics",
        "Impact-focused bullet with specific results"
      ]
    }
  ],
  "projects": [
    {
      "name": "From artifact title",
      "description": "Tailored description emphasizing job-relevant aspects",
      "technologies": ["tech1", "tech2"],
      "evidence_reference": "Reference to evidence link"
    }
  ]
}

Requirements:
- Use action verbs and quantified achievements
- Emphasize skills mentioned in job requirements
- Keep bullet points concise (1-2 lines max)
- Include evidence references where relevant
- Maintain professional tone
"""
```

### Model Integration Interfaces
```python
# Provider Abstraction
class LLMProvider(ABC):
    @abstractmethod
    async def complete(
        self,
        prompt: str,
        model: str,
        max_tokens: int = 2000,
        temperature: float = 0.7,
        **kwargs
    ) -> LLMResponse:
        pass

    @abstractmethod
    async def embed(self, text: str) -> List[float]:
        pass

# OpenAI Provider Implementation
class OpenAIProvider(LLMProvider):
    def __init__(self, api_key: str):
        self.client = AsyncOpenAI(api_key=api_key)

    async def complete(self, prompt: str, model: str = "gpt-4", **kwargs) -> LLMResponse:
        try:
            response = await self.client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=kwargs.get("max_tokens", 2000),
                temperature=kwargs.get("temperature", 0.7)
            )

            return LLMResponse(
                content=response.choices[0].message.content,
                model=model,
                tokens_used=response.usage.total_tokens,
                finish_reason=response.choices[0].finish_reason
            )
        except Exception as e:
            raise LLMProviderError(f"OpenAI API error: {str(e)}")

# Response Types
@dataclass
class LLMResponse:
    content: str
    model: str
    tokens_used: int
    finish_reason: str
    response_time: float
    provider: str
```

### Task-Specific Interfaces
```python
# Job Description Parsing
@dataclass
class ParsedJobDescription:
    role_title: str
    company_info: CompanyInfo
    requirements: JobRequirements
    responsibilities: List[str]
    company_values: List[str]
    benefits: List[str]
    parsing_confidence: float

class JobDescriptionParser:
    async def parse(self, job_description: str) -> ParsedJobDescription:
        prompt = self.prompt_manager.render("parse_jd", {
            "job_description": job_description
        })

        response = await self.model_router.complete(
            prompt=prompt,
            task_type="parsing",
            timeout=30
        )

        return self.response_parser.parse_jd_response(response)

# Skill Matching
@dataclass
class SkillMatch:
    artifact_id: int
    relevance_score: float  # 0-1
    matched_skills: List[str]
    missing_skills: List[str]
    explanation: str

class SkillMatcher:
    async def match_artifacts(
        self,
        artifacts: List[Artifact],
        job_requirements: JobRequirements
    ) -> List[SkillMatch]:

        # Embed job requirements and artifact descriptions
        job_embedding = await self.embedding_provider.embed(
            self._format_requirements(job_requirements)
        )

        matches = []
        for artifact in artifacts:
            artifact_embedding = await self.embedding_provider.embed(
                self._format_artifact(artifact)
            )

            similarity = cosine_similarity(job_embedding, artifact_embedding)

            # Use LLM for detailed skill analysis
            detailed_match = await self._analyze_skill_match(artifact, job_requirements)

            matches.append(SkillMatch(
                artifact_id=artifact.id,
                relevance_score=similarity,
                matched_skills=detailed_match.matched_skills,
                missing_skills=detailed_match.missing_skills,
                explanation=detailed_match.explanation
            ))

        return sorted(matches, key=lambda x: x.relevance_score, reverse=True)
```

## Data & Storage

### Prompt Version Management
```python
# Prompt Storage Schema
class PromptVersion(models.Model):
    id = models.UUIDField(primary_key=True, default=uuid.uuid4)
    name = models.CharField(max_length=100)
    version = models.CharField(max_length=20)  # semver: 1.0.0
    task_type = models.CharField(max_length=50)
    template = models.TextField()
    input_schema = models.JSONField()
    output_schema = models.JSONField()
    model_params = models.JSONField(default=dict)
    is_active = models.BooleanField(default=False)
    performance_metrics = models.JSONField(default=dict)
    created_at = models.DateTimeField(auto_now_add=True)
    created_by = models.ForeignKey(User, on_delete=models.SET_NULL, null=True)

# Prompt Performance Tracking
class PromptExecution(models.Model):
    prompt_version = models.ForeignKey(PromptVersion, on_delete=models.CASCADE)
    input_hash = models.CharField(max_length=64)  # SHA-256 of input
    output_content = models.TextField()
    model_used = models.CharField(max_length=50)
    tokens_used = models.IntegerField()
    execution_time = models.FloatField()  # seconds
    quality_score = models.FloatField(null=True)  # 0-1
    user_rating = models.IntegerField(null=True)  # 1-10
    error_message = models.TextField(blank=True)
    created_at = models.DateTimeField(auto_now_add=True)

# Golden Dataset Management
class GoldenExample(models.Model):
    task_type = models.CharField(max_length=50)
    input_data = models.JSONField()
    expected_output = models.JSONField()
    quality_threshold = models.FloatField(default=0.8)
    tags = models.JSONField(default=list)  # ["edge_case", "common", "complex"]
    created_at = models.DateTimeField(auto_now_add=True)
    is_active = models.BooleanField(default=True)
```

### Model Response Caching
```python
# Redis Caching Strategy
import hashlib
import json
from typing import Optional

class LLMResponseCache:
    def __init__(self, redis_client, default_ttl: int = 3600):
        self.redis = redis_client
        self.default_ttl = default_ttl

    def _generate_cache_key(self, prompt: str, model: str, **params) -> str:
        """Generate deterministic cache key for prompt + model + params"""
        cache_data = {
            "prompt": prompt,
            "model": model,
            **params
        }
        cache_string = json.dumps(cache_data, sort_keys=True)
        return f"llm_cache:{hashlib.sha256(cache_string.encode()).hexdigest()}"

    async def get(self, prompt: str, model: str, **params) -> Optional[LLMResponse]:
        cache_key = self._generate_cache_key(prompt, model, **params)
        cached_data = await self.redis.get(cache_key)

        if cached_data:
            return LLMResponse.from_json(cached_data)
        return None

    async def set(
        self,
        prompt: str,
        model: str,
        response: LLMResponse,
        ttl: Optional[int] = None,
        **params
    ):
        cache_key = self._generate_cache_key(prompt, model, **params)
        await self.redis.setex(
            cache_key,
            ttl or self.default_ttl,
            response.to_json()
        )
```

### ML Dependencies with uv

#### Project Structure for LLM Services
```
llm_services/
├── pyproject.toml              # ML-specific dependencies
├── uv.lock                     # Lockfile with ML package versions
├── requirements/
│   ├── ml.txt                 # Core ML dependencies
│   ├── llm.txt                # LLM provider libraries
│   ├── evaluation.txt         # Testing and evaluation tools
│   └── dev.txt                # Development tools
├── src/
│   ├── llm_service/
│   │   ├── providers/         # LLM provider implementations
│   │   ├── evaluation/        # Quality evaluation framework
│   │   └── prompts/           # Prompt templates
│   └── tests/
└── scripts/
    ├── setup-ml-env.sh        # ML environment setup
    └── evaluate-models.sh     # Model evaluation runner
```

#### ML Dependencies Configuration (pyproject.toml)
```toml
[project]
name = "cv-tailor-llm"
version = "0.1.0"
dependencies = [
    # Core ML and data processing
    "numpy>=1.24,<2.0",
    "pandas>=2.0,<3.0",
    "scikit-learn>=1.3,<2.0",
    "scipy>=1.11,<2.0",

    # LLM Provider APIs
    "openai>=1.0,<2.0",
    "anthropic>=0.7,<1.0",
    "httpx>=0.25,<1.0",
    "aiohttp>=3.8,<4.0",

    # Text processing and NLP
    "jinja2>=3.1,<4.0",
    "pydantic>=2.0,<3.0",
    "tiktoken>=0.5,<1.0",
    "transformers>=4.30,<5.0",
    "sentence-transformers>=2.2,<3.0",

    # Vector operations and embeddings
    "faiss-cpu>=1.7,<2.0",
    "chromadb>=0.4,<1.0",

    # Async and caching
    "redis>=5.0,<6.0",
    "asyncio-throttle>=1.0,<2.0",

    # Monitoring and evaluation
    "mlflow>=2.6,<3.0",
    "wandb>=0.15,<1.0",
    "prometheus-client>=0.17,<1.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.4,<8.0",
    "pytest-asyncio>=0.21,<1.0",
    "pytest-mock>=3.11,<4.0",
    "jupyter>=1.0,<2.0",
    "notebook>=7.0,<8.0",
    "black>=23.0,<24.0",
    "isort>=5.12,<6.0",
    "mypy>=1.5,<2.0",
    "pre-commit>=3.4,<4.0",
]
evaluation = [
    "datasets>=2.14,<3.0",
    "evaluate>=0.4,<1.0",
    "rouge-score>=0.1,<1.0",
    "bert-score>=0.3,<1.0",
    "sacrebleu>=2.3,<3.0",
]
gpu = [
    "torch>=2.0,<3.0",
    "transformers[torch]>=4.30,<5.0",
    "faiss-gpu>=1.7,<2.0",
]

[tool.uv]
dev-dependencies = [
    "pytest>=7.4",
    "jupyter>=1.0",
    "black>=23.0",
]

# Pin critical ML dependencies for reproducibility
[tool.uv.sources]
torch = { version = ">=2.0,<2.1" }
transformers = { version = ">=4.30,<4.35" }
openai = { version = ">=1.0,<1.5" }
```

#### ML Development Workflow
```bash
# Initial ML environment setup
uv sync                          # Install all ML dependencies
uv sync --group evaluation       # Install evaluation dependencies
uv sync --group gpu              # Install GPU-enabled versions

# Development commands
uv run python -m llm_service.evaluate
uv run jupyter notebook
uv run python scripts/train_embeddings.py
uv run mlflow ui

# Package management
uv add torch --group gpu         # Add GPU-specific dependency
uv add --dev ipykernel          # Add Jupyter kernel for dev
uv remove outdated-package      # Remove unused ML package
uv lock                         # Update ML dependency lockfile

# Environment isolation for different experiments
uv venv experiments             # Create experiment-specific environment
uv pip install -r requirements/experimental.txt
```

#### Docker Configuration for ML Services
```dockerfile
FROM python:3.11-slim

# Install system dependencies for ML packages
RUN apt-get update && apt-get install -y \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# Install uv
RUN pip install uv

# Copy dependency files
COPY pyproject.toml uv.lock ./

# Install ML dependencies (no dev dependencies in production)
RUN uv sync --frozen --no-dev

# Copy source code
COPY src/ ./src/

# Set environment for optimal ML performance
ENV PYTHONPATH=/app/src
ENV OMP_NUM_THREADS=4
ENV MKL_NUM_THREADS=4

# Run LLM service
CMD ["uv", "run", "python", "-m", "llm_service.main"]
```

## Reliability & SLIs/SLOs

### LLM Service Level Objectives
- **Generation Success Rate:** ≥95% for valid inputs
- **Response Time:** P95 ≤30s for CV generation, P95 ≤15s for cover letters
- **Quality Score:** ≥8/10 average user rating
- **Availability:** ≥99.5% (accounting for provider SLAs)
- **Token Efficiency:** ≤10,000 tokens per CV generation on average

### Reliability Mechanisms
```python
# Circuit Breaker Implementation
class LLMCircuitBreaker:
    def __init__(self, failure_threshold: int = 5, timeout: int = 60):
        self.failure_threshold = failure_threshold
        self.timeout = timeout
        self.failure_count = 0
        self.last_failure_time = None
        self.state = "CLOSED"  # CLOSED, OPEN, HALF_OPEN

    async def call(self, func, *args, **kwargs):
        if self.state == "OPEN":
            if time.time() - self.last_failure_time > self.timeout:
                self.state = "HALF_OPEN"
            else:
                raise CircuitBreakerOpenError("Circuit breaker is OPEN")

        try:
            result = await func(*args, **kwargs)
            if self.state == "HALF_OPEN":
                self.state = "CLOSED"
                self.failure_count = 0
            return result
        except Exception as e:
            self.failure_count += 1
            self.last_failure_time = time.time()

            if self.failure_count >= self.failure_threshold:
                self.state = "OPEN"

            raise e

# Retry Logic with Exponential Backoff
async def retry_with_backoff(
    func,
    max_retries: int = 3,
    base_delay: float = 1.0,
    max_delay: float = 60.0,
    backoff_factor: float = 2.0
):
    for attempt in range(max_retries + 1):
        try:
            return await func()
        except Exception as e:
            if attempt == max_retries:
                raise e

            delay = min(base_delay * (backoff_factor ** attempt), max_delay)
            await asyncio.sleep(delay)

# Provider Failover
class ModelRouter:
    def __init__(self, providers: List[LLMProvider]):
        self.providers = providers
        self.current_provider_index = 0

    async def complete_with_failover(self, prompt: str, **kwargs) -> LLMResponse:
        last_exception = None

        for i in range(len(self.providers)):
            provider = self.providers[self.current_provider_index]

            try:
                response = await provider.complete(prompt, **kwargs)
                return response
            except Exception as e:
                last_exception = e
                logger.warning(f"Provider {provider.__class__.__name__} failed: {e}")
                self.current_provider_index = (self.current_provider_index + 1) % len(self.providers)

        raise AllProvidersFailedError(f"All providers failed. Last error: {last_exception}")
```

## Security & Privacy

### API Key Management
```python
# Secure Key Storage
import os
from cryptography.fernet import Fernet

class SecureKeyManager:
    def __init__(self):
        self.cipher = Fernet(os.environ['ENCRYPTION_KEY'].encode())

    def encrypt_key(self, api_key: str) -> str:
        return self.cipher.encrypt(api_key.encode()).decode()

    def decrypt_key(self, encrypted_key: str) -> str:
        return self.cipher.decrypt(encrypted_key.encode()).decode()

# Environment-based Configuration
class LLMConfig:
    def __init__(self):
        self.openai_api_key = os.environ.get('OPENAI_API_KEY')
        self.anthropic_api_key = os.environ.get('ANTHROPIC_API_KEY')
        self.azure_api_key = os.environ.get('AZURE_OPENAI_KEY')

        # Validate all required keys are present
        if not any([self.openai_api_key, self.anthropic_api_key]):
            raise ValueError("At least one LLM provider API key must be configured")
```

### Input Sanitization and Validation
```python
# Input Validation
from pydantic import BaseModel, validator
import re

class JobDescriptionInput(BaseModel):
    content: str
    company_name: Optional[str] = None

    @validator('content')
    def validate_content(cls, v):
        if not v or len(v.strip()) < 50:
            raise ValueError('Job description must be at least 50 characters')

        if len(v) > 50000:  # Reasonable limit for JD length
            raise ValueError('Job description too long (max 50,000 characters)')

        # Remove potential prompt injection attempts
        suspicious_patterns = [
            r'ignore.{0,20}previous.{0,20}instructions',
            r'system.{0,20}prompt',
            r'forget.{0,20}everything',
        ]

        for pattern in suspicious_patterns:
            if re.search(pattern, v, re.IGNORECASE):
                raise ValueError('Invalid content detected')

        return v.strip()

# Output Sanitization
class OutputSanitizer:
    @staticmethod
    def sanitize_cv_content(content: str) -> str:
        # Remove potential PII that shouldn't be in CV
        pii_patterns = [
            r'\b\d{3}-\d{2}-\d{4}\b',  # SSN
            r'\b\d{4}\s?\d{4}\s?\d{4}\s?\d{4}\b',  # Credit card
        ]

        sanitized = content
        for pattern in pii_patterns:
            sanitized = re.sub(pattern, '[REDACTED]', sanitized)

        return sanitized
```

## Evaluation Plan

### Golden Dataset Creation
```python
# Golden Dataset Structure
@dataclass
class GoldenExample:
    task_type: str  # 'parse_jd', 'generate_cv', etc.
    input_data: Dict[str, Any]
    expected_output: Dict[str, Any]
    quality_threshold: float
    tags: List[str]  # ['edge_case', 'common', 'technical_role']

# CV Generation Golden Examples
CV_GOLDEN_EXAMPLES = [
    GoldenExample(
        task_type="generate_cv",
        input_data={
            "job_description": "Senior Software Engineer position...",
            "artifacts": [
                {
                    "title": "E-commerce Platform",
                    "description": "Built scalable platform handling 1M+ users",
                    "technologies": ["Python", "Django", "PostgreSQL", "Redis"],
                    "evidence_links": ["https://github.com/user/ecommerce"]
                }
            ]
        },
        expected_output={
            "professional_summary": "Senior Software Engineer with 5+ years...",
            "key_skills": ["Python", "Django", "PostgreSQL", "Redis"],
            "experience": [
                {
                    "achievements": [
                        "Developed scalable e-commerce platform serving 1M+ users",
                        "Optimized database queries reducing response time by 40%"
                    ]
                }
            ]
        },
        quality_threshold=0.85,
        tags=["common", "technical_role", "backend"]
    )
]
```

### Automated Evaluation Framework
```python
# Quality Metrics Calculator
class QualityMetrics:
    @staticmethod
    def calculate_cv_quality(generated: Dict, golden: Dict) -> float:
        scores = []

        # Content relevance score
        relevance = QualityMetrics._calculate_relevance(
            generated.get('key_skills', []),
            golden.get('key_skills', [])
        )
        scores.append(('relevance', relevance, 0.3))

        # Achievement quality score
        achievement_quality = QualityMetrics._calculate_achievement_quality(
            generated.get('experience', []),
            golden.get('experience', [])
        )
        scores.append(('achievements', achievement_quality, 0.4))

        # Professional tone score
        tone_score = QualityMetrics._calculate_tone_score(
            generated.get('professional_summary', '')
        )
        scores.append(('tone', tone_score, 0.3))

        # Weighted average
        weighted_score = sum(score * weight for _, score, weight in scores)
        return weighted_score

    @staticmethod
    def _calculate_relevance(generated_skills: List[str], expected_skills: List[str]) -> float:
        if not expected_skills:
            return 1.0

        generated_set = set(skill.lower() for skill in generated_skills)
        expected_set = set(skill.lower() for skill in expected_skills)

        # Jaccard similarity
        intersection = len(generated_set & expected_set)
        union = len(generated_set | expected_set)

        return intersection / union if union > 0 else 0.0

# Automated Evaluation Runner
class EvaluationRunner:
    def __init__(self, llm_service: LLMService, golden_dataset: List[GoldenExample]):
        self.llm_service = llm_service
        self.golden_dataset = golden_dataset

    async def run_evaluation(self, prompt_version: str) -> Dict[str, Any]:
        results = []

        for example in self.golden_dataset:
            try:
                generated = await self.llm_service.process_task(
                    task_type=example.task_type,
                    input_data=example.input_data,
                    prompt_version=prompt_version
                )

                quality_score = QualityMetrics.calculate_cv_quality(
                    generated, example.expected_output
                )

                results.append({
                    'example_id': example.id,
                    'quality_score': quality_score,
                    'passed_threshold': quality_score >= example.quality_threshold,
                    'generated_output': generated,
                    'execution_time': generated.get('execution_time')
                })

            except Exception as e:
                results.append({
                    'example_id': example.id,
                    'error': str(e),
                    'passed_threshold': False
                })

        return self._aggregate_results(results)

    def _aggregate_results(self, results: List[Dict]) -> Dict[str, Any]:
        passed_count = sum(1 for r in results if r.get('passed_threshold', False))
        total_count = len(results)

        quality_scores = [r['quality_score'] for r in results if 'quality_score' in r]
        avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0

        return {
            'pass_rate': passed_count / total_count,
            'average_quality_score': avg_quality,
            'total_examples': total_count,
            'passed_examples': passed_count,
            'detailed_results': results
        }
```

### A/B Testing Framework
```python
# A/B Test Configuration
class ABTestConfig(BaseModel):
    name: str
    description: str
    traffic_split: float  # 0.0 to 1.0
    control_prompt_version: str
    experiment_prompt_version: str
    success_metrics: List[str]  # ['quality_score', 'user_rating', 'generation_time']
    minimum_sample_size: int
    statistical_significance_threshold: float = 0.05

# A/B Test Manager
class ABTestManager:
    def __init__(self, redis_client):
        self.redis = redis_client

    async def assign_variant(self, user_id: str, test_config: ABTestConfig) -> str:
        """Consistent assignment of users to test variants"""
        hash_input = f"{test_config.name}:{user_id}"
        hash_value = int(hashlib.md5(hash_input.encode()).hexdigest(), 16)

        if (hash_value % 100) / 100 < test_config.traffic_split:
            return test_config.experiment_prompt_version
        else:
            return test_config.control_prompt_version

    async def record_result(
        self,
        test_name: str,
        user_id: str,
        variant: str,
        metrics: Dict[str, float]
    ):
        """Record A/B test results for analysis"""
        result_data = {
            'user_id': user_id,
            'variant': variant,
            'metrics': metrics,
            'timestamp': time.time()
        }

        await self.redis.lpush(
            f"ab_test:{test_name}:results",
            json.dumps(result_data)
        )
```

## Rollout & Ops Impact

### Model Performance Monitoring
```python
# Real-time Metrics Collection
class LLMMetricsCollector:
    def __init__(self, prometheus_registry):
        self.registry = prometheus_registry

        # Prometheus metrics
        self.generation_duration = Histogram(
            'llm_generation_duration_seconds',
            'Time spent on LLM generation',
            ['task_type', 'model', 'provider']
        )

        self.generation_success_rate = Counter(
            'llm_generation_total',
            'Total LLM generations',
            ['task_type', 'model', 'status']
        )

        self.quality_scores = Histogram(
            'llm_quality_score',
            'Quality scores for generated content',
            ['task_type', 'model']
        )

        self.token_usage = Histogram(
            'llm_tokens_used',
            'Number of tokens used per generation',
            ['task_type', 'model', 'provider']
        )

    def record_generation(
        self,
        task_type: str,
        model: str,
        provider: str,
        duration: float,
        tokens_used: int,
        success: bool,
        quality_score: Optional[float] = None
    ):
        # Record metrics
        self.generation_duration.labels(
            task_type=task_type,
            model=model,
            provider=provider
        ).observe(duration)

        self.generation_success_rate.labels(
            task_type=task_type,
            model=model,
            status='success' if success else 'failure'
        ).inc()

        self.token_usage.labels(
            task_type=task_type,
            model=model,
            provider=provider
        ).observe(tokens_used)

        if quality_score is not None:
            self.quality_scores.labels(
                task_type=task_type,
                model=model
            ).observe(quality_score)
```

### Feature Flag Integration
```python
# LLM Feature Flags
class LLMFeatureFlags:
    def __init__(self, config_service):
        self.config = config_service

    async def get_model_for_task(self, task_type: str, user_id: str) -> str:
        """Get model based on feature flags and user assignment"""

        # Check if user is in beta group for new models
        if await self.config.is_user_in_beta(user_id, f"llm_{task_type}_gpt4_turbo"):
            return "gpt-4-turbo"

        # Default model selection
        model_map = {
            "parse_jd": await self.config.get_flag("llm.parse_jd.default_model", "gpt-3.5-turbo"),
            "generate_cv": await self.config.get_flag("llm.generate_cv.default_model", "gpt-4"),
            "generate_cover_letter": await self.config.get_flag("llm.cover_letter.default_model", "gpt-3.5-turbo"),
        }

        return model_map.get(task_type, "gpt-3.5-turbo")

    async def should_use_cache(self, task_type: str) -> bool:
        """Check if caching is enabled for task type"""
        return await self.config.get_flag(f"llm.{task_type}.cache_enabled", True)
```

## Risks & Rollback

### LLM-Specific Risk Mitigation
```python
# Content Safety Filter
class ContentSafetyFilter:
    def __init__(self):
        self.prohibited_patterns = [
            r'personally identifiable information',
            r'social security number',
            r'credit card',
            r'password',
            r'api[_\s]?key'
        ]

    def scan_output(self, content: str) -> Dict[str, Any]:
        """Scan generated content for safety issues"""
        issues = []

        for pattern in self.prohibited_patterns:
            if re.search(pattern, content, re.IGNORECASE):
                issues.append(f"Potential PII detected: {pattern}")

        # Check for hallucinated URLs/emails
        urls = re.findall(r'https?://[^\s]+', content)
        for url in urls:
            if not self._verify_url_legitimacy(url):
                issues.append(f"Potentially hallucinated URL: {url}")

        return {
            'is_safe': len(issues) == 0,
            'issues': issues,
            'content': content if len(issues) == 0 else self._sanitize_content(content)
        }

# Model Drift Detection
class ModelDriftDetector:
    def __init__(self, baseline_metrics: Dict[str, float]):
        self.baseline = baseline_metrics
        self.drift_threshold = 0.1  # 10% degradation threshold

    async def check_drift(self, current_metrics: Dict[str, float]) -> bool:
        """Check if model performance has drifted significantly"""
        for metric, baseline_value in self.baseline.items():
            current_value = current_metrics.get(metric, 0)

            if current_value < baseline_value * (1 - self.drift_threshold):
                logger.warning(
                    f"Model drift detected for {metric}: "
                    f"{current_value:.3f} < {baseline_value:.3f}"
                )
                return True

        return False

# Automatic Rollback System
class LLMRollbackManager:
    def __init__(self, prompt_manager, metrics_collector):
        self.prompt_manager = prompt_manager
        self.metrics_collector = metrics_collector

    async def check_and_rollback(self, prompt_version: str, task_type: str):
        """Check metrics and rollback if necessary"""
        current_metrics = await self.metrics_collector.get_recent_metrics(
            prompt_version, task_type, window_minutes=30
        )

        if current_metrics.get('error_rate', 0) > 0.05:  # 5% error rate
            await self.rollback_to_previous_version(prompt_version, task_type)

        if current_metrics.get('avg_quality_score', 1.0) < 0.7:  # Quality threshold
            await self.rollback_to_previous_version(prompt_version, task_type)

    async def rollback_to_previous_version(self, current_version: str, task_type: str):
        """Rollback to the previous stable version"""
        previous_version = await self.prompt_manager.get_previous_stable_version(
            task_type, current_version
        )

        if previous_version:
            await self.prompt_manager.set_active_version(task_type, previous_version)
            logger.critical(
                f"Automatic rollback executed: {task_type} "
                f"{current_version} -> {previous_version}"
            )
```

## Open Questions

1. **Model Selection Strategy:** Single provider vs multi-provider approach for different tasks
2. **Fine-tuning Requirements:** Whether to fine-tune models on domain-specific CV/resume data
3. **Cost Optimization:** Token usage optimization vs quality trade-offs
4. **Real-time vs Batch Processing:** Immediate generation vs queued processing for cost savings
5. **Embedding Model Choice:** OpenAI embeddings vs specialized models for skill matching

## Changelog

- 2025-09-23: Draft created; LLM architecture designed; prompt management system specified; evaluation framework established; quality metrics and monitoring defined; uv dependency management for ML packages added