# LLM Services Test Suite

Comprehensive test suite for the LLM Services Django app, covering models, services, views, tasks, serializers, and real API integrations.

## Test Structure

```
tests/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ conftest.py              # Pytest configuration and fixtures
â”œâ”€â”€ test_models.py           # Model tests
â”œâ”€â”€ test_services.py         # Service class tests
â”œâ”€â”€ test_views.py            # API view tests
â”œâ”€â”€ test_tasks.py            # Celery task tests
â”œâ”€â”€ test_serializers.py      # DRF serializer tests
â”œâ”€â”€ test_real_*.py           # Real API integration tests
â”œâ”€â”€ run_real_api_tests.py    # Real API test runner
â””â”€â”€ README.md               # This file
```

## Running Tests

### All Tests
```bash
cd backend
uv run pytest llm_services/tests/
```

### Specific Test Files
```bash
uv run pytest llm_services/tests/test_models.py
uv run pytest llm_services/tests/test_services.py
uv run pytest llm_services/tests/test_views.py
uv run pytest llm_services/tests/test_tasks.py
uv run pytest llm_services/tests/test_serializers.py
```

### With Coverage
```bash
uv run pytest llm_services/tests/ --cov=llm_services --cov-report=html
```

### Real API Integration Tests

For tests that use actual LLM APIs (OpenAI, Anthropic) with budget controls:

```bash
# Check configuration and API keys
uv run python llm_services/tests/run_real_api_tests.py --check-config

# Run basic real API tests (~$0.10)
uv run python llm_services/tests/run_real_api_tests.py --run-basic --max-cost=0.10 --force

# Run all real API tests (~$0.35)
uv run python llm_services/tests/run_real_api_tests.py --run-all --max-cost=0.50 --force

# Run specific real API test
uv run python llm_services/tests/run_real_api_tests.py --run-single test_real_job_description_parsing
```

âš ï¸ **Warning**: Real API tests use actual API tokens and incur costs. See main TESTING.md for complete documentation.

### Specific Test Classes or Methods
```bash
uv run pytest llm_services/tests/test_models.py::ModelPerformanceMetricTestCase
uv run pytest llm_services/tests/test_services.py::EnhancedLLMServiceTestCase::test_parse_job_description
```

### Test Markers
```bash
uv run pytest -m unit                    # Run only unit tests
uv run pytest -m integration             # Run only integration tests
uv run pytest -m "not slow"             # Skip slow tests
uv run pytest -m async_test             # Run only async tests
```

## Test Coverage

### Models (test_models.py)
- âœ… `ModelPerformanceMetric` - Creation, validation, string representation
- âœ… `EnhancedArtifact` - Creation, relationships, vector field mocking
- âœ… `ArtifactChunk` - Creation, unique constraints, relationships
- âœ… `JobDescriptionEmbedding` - Creation, unique constraints, access tracking
- âœ… `ModelCostTracking` - Creation, unique constraints, aggregation
- âœ… `CircuitBreakerState` - Creation, state transitions, request validation

### Services (test_services.py)
- âœ… `EnhancedLLMService` - Model selection, job parsing, CV generation
- âœ… `FlexibleEmbeddingService` - Embedding generation, artifact storage, job caching
- âœ… `AdvancedDocumentProcessor` - PDF processing, text chunking, LangChain integration
- âœ… `ModelPerformanceTracker` - Performance recording, statistics, model selection
- âœ… `CircuitBreakerService` - Success/failure recording, request validation
- âœ… `ModelRegistry` - Model configuration, filtering, availability

### Views (test_views.py)
- âœ… `ModelPerformanceMetricViewSet` - CRUD operations, filtering, summaries
- âœ… `CircuitBreakerStateViewSet` - State management, health monitoring
- âœ… `ModelCostTrackingViewSet` - Cost analysis, monthly summaries
- âœ… `JobDescriptionEmbeddingViewSet` - Cache management, statistics
- âœ… `EnhancedArtifactViewSet` - Artifact management, chunk access
- âœ… Custom API views - Model stats, selection, health, available models

### Tasks (test_tasks.py)
- âœ… `generate_cv_task` - End-to-end CV generation workflow
- âœ… `generate_job_embedding_cache` - Job embedding caching
- âœ… `enhance_artifact_with_llm` - Artifact processing with retries
- âœ… `cleanup_expired_generations` - Data cleanup tasks
- âœ… `cleanup_old_performance_metrics` - Performance data cleanup

### Serializers (test_serializers.py)
- âœ… All DRF serializers - Serialization, deserialization, validation
- âœ… Custom serializers - Model stats, selection requests/responses
- âœ… Field validation - Write-only fields, calculated fields, constraints

## Mock Strategy

### External Dependencies Mocked
- ğŸ”§ **OpenAI API** - Chat completions, embeddings, token usage
- ğŸ”§ **Anthropic API** - Claude model interactions
- ğŸ”§ **LangChain** - Document loaders, text splitters, processing
- ğŸ”§ **pgvector** - Vector field operations, similarity search
- ğŸ”§ **Redis Cache** - Caching operations
- ğŸ”§ **File System** - Document file access, uploads

### Fixtures Available
- `user`, `staff_user` - Test user accounts
- `mock_openai_client` - Mocked OpenAI client with responses
- `mock_langchain` - Mocked LangChain components
- `mock_pgvector` - Mocked vector field operations
- `performance_metrics` - Test performance data
- `circuit_breakers` - Test circuit breaker states
- `job_embeddings` - Test embedding data
- `enhanced_artifacts` - Test artifact and chunk data

## Error Testing

### Comprehensive Error Scenarios
- âœ… **API Failures** - Network errors, invalid responses, rate limits
- âœ… **Validation Errors** - Invalid data, constraint violations
- âœ… **Processing Errors** - Document parsing failures, embedding errors
- âœ… **Database Errors** - Constraint violations, connection issues
- âœ… **Authentication Errors** - Unauthorized access, permission denied
- âœ… **Circuit Breaker Logic** - Failure thresholds, timeout handling

## Performance Testing

### Async Operations Tested
- âœ… Job description parsing with LLM calls
- âœ… CV content generation with multiple API calls
- âœ… Embedding generation and storage
- âœ… Document processing with LangChain
- âœ… Bulk operations and cleanup tasks

## Configuration

### Test Settings
- Uses in-memory SQLite database for speed
- Mocks external API calls to avoid costs
- Disables cache for deterministic results
- Uses dummy cache backend
- Sets test API keys to avoid missing key errors

### Environment Variables for Testing
```bash
export DJANGO_SETTINGS_MODULE=cv_tailor.settings
export DATABASE_URL=sqlite://:memory:
export OPENAI_API_KEY=test-key
export ANTHROPIC_API_KEY=test-key
```

## Best Practices Followed

### Test Organization
- âœ… **Arrange-Act-Assert** pattern consistently applied
- âœ… **Single responsibility** - Each test focuses on one behavior
- âœ… **Descriptive names** - Test names clearly describe what's being tested
- âœ… **Proper setup/teardown** - Using fixtures and Django TestCase
- âœ… **Mock isolation** - External dependencies properly mocked

### Coverage Goals
- ğŸ¯ **Models**: 100% line coverage, all methods tested
- ğŸ¯ **Services**: 95%+ coverage, core business logic thoroughly tested
- ğŸ¯ **Views**: 90%+ coverage, all endpoints and error cases tested
- ğŸ¯ **Tasks**: 90%+ coverage, async operations and error handling tested
- ğŸ¯ **Serializers**: 95%+ coverage, validation and field behavior tested

## Running in CI/CD

```yaml
# Example GitHub Actions test job
test-llm-services:
  runs-on: ubuntu-latest
  steps:
    - uses: actions/checkout@v3
    - name: Install uv
      uses: astral-sh/setup-uv@v1
    - name: Run LLM Services Tests
      run: |
        cd backend
        uv run pytest llm_services/tests/ \
          --cov=llm_services \
          --cov-report=xml \
          --junit-xml=test-results.xml
```

## Troubleshooting

### Common Issues
1. **Import Errors** - Ensure all dependencies are installed via `uv sync`
2. **Mock Issues** - Check that external services are properly mocked
3. **Database Issues** - Ensure test database permissions are correct
4. **Async Issues** - Use `@pytest.mark.asyncio` for async tests

### Debug Commands
```bash
# Run with verbose output
uv run pytest llm_services/tests/ -v -s

# Run specific failing test with full traceback
uv run pytest llm_services/tests/test_services.py::test_name -vvv --tb=long

# Run with pdb debugger on failures
uv run pytest llm_services/tests/ --pdb
```